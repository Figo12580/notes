## 信息论与编码基础

（国防科大b站课程）

[TOC]

### 绪论

### 信息的定义——Shannon信息

消息、知识、学问、数据……我们直观认为，信息的外延和相关概念非常丰富，但是其内涵是不容易定义的。

信息不完全是一个人为定义的学科概念。虽然信息论提供了比较完备的逻辑框架，似乎生硬地规定了信息的边界，但是它依然希望描述、贴近我们直观感受到的”信息“概念。换言之，我们冥冥中便知道”信息“的存在，可以直观感受到其部分特征，但是很难用理性去描述和把握。信息论就提供了这种途径，所以它是通信的哲学。

>  在通信理论中，我们提取我们需要的信息特征，使用统计语言描述信息。这就是通信理论和信息论作为学科对信息的定义，即<u>**统计信息**</u>。

直观上，我们认为“信息”是“有意义的新知”，它是人构建新认识的材料、过程和工具预备（在媒介里储存和传输），并且是新知中最纯粹的部分（不是事实的简单堆砌）。消除疑惑、构建认识、从无到有的过程，Shannon认为这就是信息的本质。

>  **<u>信息是事物运动状态或存在方式的不确定性的描述（Shannon信息）。</u>**

消息：信息作为”实体“存在的媒介；信号：通信系统中承载信息的物理量。

通信系统就是为了实现信息快速、保真传输的系统。

#### 信息测度

信息需要定量的测度。合理测度要符合我们的直观认识和要求：

- 不确定性（信息）是随机事件概率的单调减函数
- 值域：$[\ 0,\ \infin)$
- 加和性：两个独立随机事件的不确定性是单个独立事件不确定性的和

结合概率函数的特性与不确定性测度的要求，我们发现不确定性可以是概率的对数。

>  **<u>随机事件的自信息</u>**：
>
> $I(a)= -log_n P (a) $
>
> 随机事件的自信息是随机事件的属性，它代表随机事件的不确定性，同时也是我们缺纸该随机事件发生以后获得的信息的量。
>
> 自信息单位由对数的底决定，常用底为2，单位为bit。
>
> 推广：
>
> - 联合自信息：与联合概率对应的自信息$I(a,b)= -log_n P (a,b) $
> - 条件自信息：与条件概率对应的自信息$I(a|b)= -log_n P (a|b) $

#### 信息论的发展状况

信息论的产生离不开十九世纪电气化通信技术的发展。十九世纪，电报、电话、无线电通信相继出现，实体化通信网络日渐复杂和逻辑化。

作为通信工程师和学者，需要解决各种问题：通信设备和形式（如上文）；通信距离；信道利用率（滤波器与频分复用；振幅调制信号；单边带调制；限带信号的采样定理与时分复用），这些理论都暗示着信息的统一性

学者又逐渐关注通信过程的随机性和噪声问题；控制论和Shannon的信息论也出现了。

随后是人们基于信息论对信源编码（压缩）和信道编码（抗干扰）方法的广泛研究。

近代：网络信息论，MIMO技术，协同通信、物理层安全途径……